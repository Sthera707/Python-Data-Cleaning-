{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b94ea01",
   "metadata": {},
   "source": [
    "Project_1: Introduction to Analytics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48140e17",
   "metadata": {},
   "source": [
    "In any setting the fist thing you must do is to know and understand the players, this rule holds true even in the data sapce, they are many tools available and that can be very intimadating, like for instance you can learn SQL,Spark, SaaS, R, Python, Teblue, Powerbi, Excel. In this series of projects we will only focus on SQL,Powerbi and Python, the aim of the projects is to discover the world of data analytics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c19fdce",
   "metadata": {},
   "source": [
    "Import data and libraries: Preparation Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18cc529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #calculations\n",
    "import pandas as pd #main man\n",
    "import matplotlib.pyplot as plt #visualisation\n",
    "import seaborn as sns #visualisation\n",
    "import sklearn #machine learning\n",
    "\n",
    "file_path = r'C:\\Users\\SthembisoM\\Desktop\\Karl\\Driver score card\\Trackmatic details.csv' #machine directory\n",
    "\n",
    "df = pd.read_csv(file_path) #call the data to Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1159ee74",
   "metadata": {},
   "source": [
    "In some cases you find that the data comes in 12 different sheets and you need to Concanate the data, below is the code that will help in doing that with ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a055d19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = r'C:\\Users\\SthembisoM\\Desktop\\Project\\Guided Projects\\Sales\\SalesAnalysis\\Sales_Data'#machine directory\n",
    "\n",
    "files = [file for file in os.listdir(directory_path)]\n",
    "all_months_data = pd.DataFrame()\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(directory_path, file)  # Use os.path.join for path concatenation\n",
    "    df = pd.read_csv(file_path)\n",
    "    all_months_data = pd.concat([all_months_data, df], ignore_index=True)\n",
    "\n",
    "all_months_data.to_csv(\"all_data.csv\", index= False) #This saves the data back to your machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f8a73d",
   "metadata": {},
   "source": [
    "Data Exploration: Check missing values and duplicates on the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5ae68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows = df[df.duplicated()]\n",
    "if not duplicate_rows.empty:\n",
    "    print(\"Duplicate rows found:\")\n",
    "    print(duplicate_rows)\n",
    "else:\n",
    "    print(\"No duplicate rows found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db271f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isna().sum()\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495e7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Shipment'])\n",
    "df = df.dropna(subset=['Driver', 'Customer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85294cc2",
   "metadata": {},
   "source": [
    "At this stage your data is clean and you might fel ready to jump and do your statistical analysis and plot you graphs but there is one pitfall, that brings frustrations and syntax errors form pythonn, data types, I will not explain data types but its critical that you understand it and you ensure your data is in the correct data type either you working on python, SQL, Excel or Powerbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de5b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns with timestamps to convert\n",
    "timestamp_columns = ['Requested Delivery Date', 'Planned Route Start', 'Planned Route End', 'Actual Route Start', 'Actual Route End', 'Actual Arrival', 'Planned Departure', 'Actual Departure', 'Planned MST', 'Actual TAT', 'Planned Arrival']\n",
    "\n",
    "# Loop through each timestamp column and convert its data type\n",
    "for column in timestamp_columns:\n",
    "    df[column] = pd.to_datetime(df[column], errors='coerce')\n",
    "\n",
    "# Optional: Check the data types of the columns after conversion\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6ed6b2",
   "metadata": {},
   "source": [
    "Once your data is clean and your columns are in a correct data type, you will now be able to answer the questions that you always had troubling you or your team. In the next series I will be doing a real life problem so I can demonstrate how you answer questions with data, if you read until here, leave me a comment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d06c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
